# Tech Challenge 02 - Pipeline de Dados IBOVESPA

Este projeto implementa um pipeline de dados para coleta, processamento e an√°lise de dados do IBOVESPA, utilizando AWS S3 para armazenamento e Amazon Athena para an√°lises.

## üèóÔ∏è Arquitetura do Projeto

```
techchallenge_02/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ 01_web_scraper.py      # Coleta de dados do site B3
‚îÇ   ‚îú‚îÄ‚îÄ 02_data_processor.py   # Processamento e transforma√ß√£o dos dados
‚îÇ   ‚îú‚îÄ‚îÄ 03_s3_client.py        # Upload para AWS S3
‚îÇ   ‚îî‚îÄ‚îÄ ibov_analysis.ipynb    # Notebook para an√°lise no Athena
‚îú‚îÄ‚îÄ data/                      # Dados locais (tempor√°rio)
‚îú‚îÄ‚îÄ docker-compose.yml         # Configura√ß√£o MinIO (desenvolvimento)
‚îú‚îÄ‚îÄ Dockerfile                 # Container da aplica√ß√£o
‚îú‚îÄ‚îÄ requirements.txt           # Depend√™ncias Python
‚îî‚îÄ‚îÄ README.md                  # Documenta√ß√£o
```

## üîÑ Fluxo de Execu√ß√£o

### 1. **Coleta de Dados** (`01_web_scraper.py`)
- Acessa o site da B3 (Brasil, Bolsa, Balc√£o)
- Extrai dados da carteira te√≥rica do IBOVESPA
- Salva dados brutos em formato JSON local
- **Dados coletados:**
  - C√≥digo da empresa
  - Tipo de a√ß√£o (ON, PN, etc.)
  - Quantidade te√≥rica
  - Participa√ß√£o percentual

### 2. **Processamento de Dados** (`02_data_processor.py`)
- Carrega dados JSON coletados
- Limpa e normaliza os dados
- Adiciona campos calculados (dias desde coleta, totais)
- Converte para formato Parquet otimizado
- **Transforma√ß√µes aplicadas:**
  - Padroniza√ß√£o de nomes de colunas
  - Convers√£o de tipos de dados
  - C√°lculo de m√©tricas agregadas
  - Valida√ß√£o de dados

### 3. **Upload para S3** (`03_s3_client.py`)
- Conecta ao AWS S3 usando boto3
- Cria estrutura de pastas hier√°rquica
- Faz upload dos arquivos Parquet processados
- **Estrutura no S3:**
  ```
  s3://ibovtech/
  ‚îú‚îÄ‚îÄ raw/
  ‚îÇ   ‚îî‚îÄ‚îÄ AAAAMMDD/           # Data da coleta
  ‚îÇ       ‚îî‚îÄ‚îÄ ibovespa.parquet # Apenas 1 arquivo por data
  ‚îú‚îÄ‚îÄ refined/                 # Dados processados para an√°lise
  ‚îî‚îÄ‚îÄ athena-results/         # Resultados das queries Athena
  ```

### 4. **An√°lise de Dados** (`ibov_analysis.ipynb`)
- Conecta ao Amazon Athena
- Executa queries SQL nos dados do S3
- Gera visualiza√ß√µes e relat√≥rios
- **An√°lises dispon√≠veis:**
  - Top 10 a√ß√µes por quantidade te√≥rica
  - Distribui√ß√£o por tipo de a√ß√£o
  - Evolu√ß√£o temporal das posi√ß√µes

## üöÄ Como Executar

### Configura√ß√£o de Ambiente

1. **Clone o reposit√≥rio:**
```bash
git clone <repository-url>
cd techchallenge_02
```

2. **Configure as vari√°veis de ambiente:**
```bash
export AWS_ACCESS_KEY_ID="sua-access-key"
export AWS_SECRET_ACCESS_KEY="sua-secret-key"
export AWS_DEFAULT_REGION="us-east-1"
export AWS_BUCKET_NAME="ibovtech"
export TZ="America/Sao_Paulo"
```

### Execu√ß√£o Local

1. **Instale as depend√™ncias:**
```bash
pip install -r requirements.txt
```

2. **Execute o pipeline completo:**
```bash
# Passo 1: Coleta de dados
python src/01_web_scraper.py

# Passo 2: Processamento
python src/02_data_processor.py

# Passo 3: Upload para S3
python src/03_s3_client.py

# Passo 4: An√°lise (Jupyter Notebook)
jupyter notebook src/ibov_analysis.ipynb
```

### Execu√ß√£o com Docker/Podman

1. **Build da imagem:**
```bash
podman build -t techchallenge-app .
```

2. **Execu√ß√£o do container:**
```bash
podman run --name scraper-b3 \
  -d \
  -e AWS_ACCESS_KEY_ID= \
  -e AWS_SECRET_ACCESS_KEY= \
  -e TZ=America/Sao_Paulo \
  quay.io/parraes/techchallenge_02:v2
```


## üìä Estrutura de Dados

### Schema da Tabela IBOVESPA

| Campo | Tipo | Descri√ß√£o |
|-------|------|-----------|
| `dias_desde_coleta` | int | Dias desde a √∫ltima coleta |
| `empresa` | string | C√≥digo da empresa |
| `tipo` | string | Tipo de a√ß√£o (ON, PN, etc.) |
| `quantidade_teorica` | bigint | Quantidade te√≥rica da a√ß√£o |
| `qtd_teorica_total` | bigint | Quantidade te√≥rica total |

### Localiza√ß√£o dos Dados no S3

- **Dados brutos:** `s3://ibovtech/raw/YYYYMMDD/`
- **Dados refinados:** `s3://ibovtech/refined/`
- **Resultados Athena:** `s3://ibovtech/athena-results/`

## üõ†Ô∏è Tecnologias Utilizadas

- **Python 3.9+** - Linguagem principal
- **AWS S3** - Armazenamento de dados
- **Amazon Athena** - Engine de consultas SQL
- **Boto3** - SDK AWS para Python
- **Pandas** - Manipula√ß√£o de dados
- **PyArrow** - Processamento Parquet
- **BeautifulSoup4** - Web scraping
- **Matplotlib/Seaborn** - Visualiza√ß√µes
- **Docker/Podman** - Containeriza√ß√£o

## üìà Monitoramento e Logs

O projeto inclui logging detalhado em todos os scripts:

- **N√≠vel INFO:** Opera√ß√µes normais e progresso
- **N√≠vel ERROR:** Erros e exce√ß√µes
- **N√≠vel DEBUG:** Informa√ß√µes detalhadas de debug

Logs s√£o enviados para stdout e podem ser coletados pelo sistema de logging do container.

## üîß Configura√ß√µes Avan√ßadas

### Personaliza√ß√£o do Bucket S3
```python
AWS_BUCKET_NAME = os.getenv("AWS_BUCKET_NAME", "ibovtech")
```

### Configura√ß√£o de Regi√£o
```python
AWS_DEFAULT_REGION = os.getenv("AWS_DEFAULT_REGION", "us-east-1")
```

### Estrutura de Pastas Customizada
O script `03_s3_client.py` permite personalizar a estrutura de pastas:
- Pasta raiz: `raw/`
- Subpastas por data: `AAAAMMDD/`
- Apenas 1 arquivo Parquet por data (substitui√ß√£o autom√°tica)



## üìÑ Licen√ßa

Este projeto est√° sob a licen√ßa MIT. Veja o arquivo LICENSE para mais detalhes.